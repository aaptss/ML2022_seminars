{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2022_seminars/blob/master/seminar6/Adaboost2022_Seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa2rPyOoHh3W"
   },
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn \n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9vNEXNSHh3d"
   },
   "source": [
    "# Adaboost\n",
    "\n",
    "## Boosting\n",
    "\n",
    "The underlying idea of boosting is to combine **cheap weak**\n",
    "predictiors into a strong **powerful ensemble**.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Eiji_Hato/publication/264713074/figure/fig5/AS:614118558023749@1523428682764/Concept-of-AdaBoost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUDatGA_Hh3f"
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "This algorithm uses a greedy strategy to construct an additive ensemble of size $T$\n",
    "from a dictionary of basis predictors.\n",
    "\n",
    "The size of the ensemble $T$ is a regularization parameter:\n",
    "* the greater the $T$, the more boosting overfits.\n",
    "\n",
    "**`AdaBoost.M1`**:\n",
    "\n",
    "![alt text](https://i.imgur.com/im1cMeS.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HG3IU9mHh3g"
   },
   "source": [
    "#### AdaBoost in SciKit\n",
    "\n",
    "The `ensemble` submodule implements both classification and regression\n",
    "versions of Adaboost\n",
    "* AdaBoostClassifier\n",
    "* AdaBoostRegressor\n",
    "\n",
    "Algorithms differ only in the **loss** used in determining the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmmZ92hNHh3h"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE9cJo9eHh3k"
   },
   "source": [
    "### Adaboost params\n",
    "\n",
    "Common parameters:\n",
    "- **n_estimators** -- the maximum size of the ensemble (in **case of a perfect fit** boosting stops earlier);\n",
    "- **base_estimator** -- the base estimator of the ensemble, which must support **sample weighting**;\n",
    "- **learning_rate** -- scales each update, slowing down convergence, but making it more accurate.\n",
    "\n",
    "AdaBoostClassifier only:\n",
    "- **algorithm** -- the AdaBoost version to use:\n",
    "    * _\"SAMME\"_ -- the SAMME (**M1**) discrete boosting algorithm;\n",
    "    * _\"SAMME.R\"_ -- the SAMME.R real boosting algorithm, typically achieves a lower test error with fewer boosting iterations than `SAMME`.\n",
    "    \n",
    "AdaBoostRegressor only:\n",
    "- **loss** -- the loss function to use when updating the weights after each boosting iteration:\n",
    "    * _\"linear\"_ -- absolute loss $L(y, p) = |y-p|$;\n",
    "    * _\"square\"_ -- squared loss $L(y, p) = |y-p|^2$;\n",
    "    * _\"exponential\"_ -- Exponential loss L(y, p) = 1-e^{-|y-p|}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTEsW4B2Hh3k"
   },
   "source": [
    "# Practice: toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jBAauIMHh3l"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, accuracy_score, mean_absolute_error, mean_squared_error, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JcpR-VBHh3o"
   },
   "outputs": [],
   "source": [
    "def scikit_example(n_samples, random_state=None):\n",
    "    X1, y1 = make_gaussian_quantiles(cov=2., n_samples=int(0.4*n_samples),\n",
    "                                     n_features=2, n_classes=2,\n",
    "                                     random_state=random_state)\n",
    "    X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                     n_samples=int(0.6*n_samples),\n",
    "                                     n_features=2, n_classes=2,\n",
    "                                     random_state=random_state)\n",
    "    return np.concatenate((X1, X2)), np.concatenate((y1, 1 - y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s48R5vYpHh3r"
   },
   "outputs": [],
   "source": [
    "X, y = scikit_example(n_samples=500, random_state=random_state)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, random_state=random_state)\n",
    "\n",
    "\n",
    "min_, max_ = np.min(X, axis=0) - 1, np.max(X, axis=0) + 1\n",
    "xx, yy = np.meshgrid(np.linspace(min_[0], max_[0], num=51),\n",
    "                     np.linspace(min_[1], max_[1], num=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdCWrCu2Hh3v"
   },
   "outputs": [],
   "source": [
    "def plot_adaboost(adaboost=None):\n",
    "    mpl.style.use('seaborn')\n",
    "    colors = ['blue', 'green' , 'red']\n",
    "\n",
    "    ax = plt.figure(figsize=(12, 8)).add_subplot(111)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=mpl.colors.ListedColormap(colors))\n",
    "\n",
    "    if adaboost:\n",
    "        prob_ = adaboost.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, prob_, alpha=0.4, cmap=plt.cm.coolwarm)\n",
    "        ax.set_title(\"AdaBoost with %d trees\"%(adaboost.n_estimators,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xjc_fahHh3y"
   },
   "outputs": [],
   "source": [
    "plot_adaboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuJ_SeTdHh31"
   },
   "outputs": [],
   "source": [
    "abc1_ = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),\n",
    "                          n_estimators=5, random_state=random_state)\n",
    "abc1_.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "plot_adaboost(abc1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us_pkn8PHh34"
   },
   "outputs": [],
   "source": [
    "abc2_ = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),\n",
    "                          n_estimators=100, random_state=random_state)\n",
    "abc2_.fit(X_train, y_train)\n",
    "\n",
    "plot_adaboost(abc2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7B2VrG8UHh36"
   },
   "outputs": [],
   "source": [
    "abc3_ = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),\n",
    "                          n_estimators=1000, random_state=random_state)\n",
    "abc3_.fit(X_train, y_train)\n",
    "\n",
    "plot_adaboost(abc2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIMgBQGVHh3-"
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(12, 8)).add_subplot(111)\n",
    "for est_ in abc1_.estimators_:\n",
    "    fpr, tpr, _ = roc_curve(y_test, est_.predict_proba(X_test)[:, 1])\n",
    "    ax.plot(fpr, tpr, color=\"red\", alpha=0.5)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, abc1_.predict_proba(X_test)[:, 1])\n",
    "ax.plot(fpr, tpr, color=\"black\", label=\"AdaBoost %d\"%(abc1_.n_estimators,))\n",
    "\n",
    "ax.legend(fontsize=\"xx-large\")\n",
    "ax.set_title(\"ROC curve for AdaBoost\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGY_PoIHHh4B"
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(12, 8)).add_subplot(111)\n",
    "for est_ in abc2_.estimators_:\n",
    "    fpr, tpr, _ = roc_curve(y_test, est_.predict_proba(X_test)[:, 1])\n",
    "    ax.plot(fpr, tpr, color=\"red\", alpha=0.05)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, abc2_.predict_proba(X_test)[:, 1])\n",
    "ax.plot(fpr, tpr, color=\"black\", label=\"AdaBoost %d\"%(abc2_.n_estimators,))\n",
    "\n",
    "ax.legend(fontsize=\"xx-large\")\n",
    "ax.set_title(\"ROC curve for AdaBoost\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2oSq_6GHh4D"
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(12, 8)).add_subplot(111)\n",
    "for est_ in abc3_.estimators_:\n",
    "    fpr, tpr, _ = roc_curve(y_test, est_.predict_proba(X_test)[:, 1])\n",
    "    ax.plot(fpr, tpr, color=\"red\", alpha=0.05)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, abc3_.predict_proba(X_test)[:, 1])\n",
    "ax.plot(fpr, tpr, color=\"black\", label=\"AdaBoost %d\"%(abc3_.n_estimators,))\n",
    "\n",
    "ax.legend(fontsize=\"xx-large\")\n",
    "ax.set_title(\"ROC curve for AdaBoost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20EFCtZSHh4G"
   },
   "source": [
    "# Adult dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1quX-G5jHh4H"
   },
   "source": [
    "Download the dataset. If you use Windows, you can download it manually by the link [https://github.com/amueller/advanced_training/raw/master/data/adult.csv](https://github.com/amueller/advanced_training/raw/master/data/adult.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w5D-RlhHh4J"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/amueller/advanced_training/raw/master/data/adult.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCj0K4SUHh4M"
   },
   "source": [
    "Let's look at it. We have some information about different persons. Our target value will be the income column. Since we have the information whether the income is above or below 50K, we will solve binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mazsUS6KHh4N"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('adult.csv', index_col=0)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeC68rflHh4U"
   },
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "(data.astype(str) == ' ?').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huRr6ednHh4W"
   },
   "outputs": [],
   "source": [
    "data = data[data.workclass != ' ?']\n",
    "data = data[data.occupation != ' ?']\n",
    "data = data[data['native-country'] != ' ?']\n",
    "\n",
    "# data = data[np.logical_and(data.workclass != ' ?', data.occupation != ' ?')]\n",
    "\n",
    "size_after = data.shape[0]\n",
    "\n",
    "(data.astype(str) == '?').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E54bdFmpHh4Y"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbzCGGfkHh4b"
   },
   "source": [
    "Let's conver our target income to binary value (0 and 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TExEBhiwHh4c"
   },
   "outputs": [],
   "source": [
    "if 'income' in data.columns:\n",
    "    y = (data['income'] == ' >50K').values.astype(np.int32)\n",
    "    del data['income']\n",
    "    \n",
    "del data['native-country']\n",
    "    \n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkYF7_JqHh4f"
   },
   "source": [
    "We have some categorical data in our dataset. Let us use one-hot-encoding for this columns. In order to automaticalli convert the data you should use method pd.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8StJ0u-JHh4g"
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88qB6THmHh4i"
   },
   "outputs": [],
   "source": [
    "data.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlIbk5ZXHh4l"
   },
   "source": [
    "# should we scale our data before AdaBoost?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaYNpVkrHh4l"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X = scale(data.astype(np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSJq4QrZHh4o"
   },
   "source": [
    "!!! Don't change random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onPgbo2zHh4o"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=9, stratify=y)\n",
    "\n",
    "X_val, X_test, y_val, y_test = \\\n",
    "    train_test_split(X_test, y_test, test_size=0.5, random_state=9, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYfVD2nKHh4r"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Roo7khE5Hh4t"
   },
   "outputs": [],
   "source": [
    "def get_test_train_errors(_adaboost):\n",
    "    train_errors = []\n",
    "\n",
    "    for y in _adaboost.staged_predict(X_train):\n",
    "        train_errors.append(f1_score(y_train, y))\n",
    "\n",
    "    test_errors = []\n",
    "\n",
    "    for y in _adaboost.staged_predict(X_val):\n",
    "        test_errors.append(f1_score(y_val, y))\n",
    "        \n",
    "    print('F1 validation score:', f1_score(y_val, _adaboost.predict(X_val)))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_errors, label='train')\n",
    "    plt.plot(test_errors, label='test')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4NhG82kHh4x"
   },
   "source": [
    "## F1 score\n",
    "\n",
    "Since we consider binary classification, it is good to use some proper measure. One of them — F1\n",
    "\n",
    "\n",
    "$$F1 = 2 * (precision * recall) / (precision + recall)$$\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdJHLa3rHh4y"
   },
   "source": [
    "Let's train AdaBoost with a lot of small trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rx_35NuqHh4z"
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=1000)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "get_test_train_errors(adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD6rVFv0Hh42"
   },
   "source": [
    "Now, increase the depth of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxWyiy1VHh43"
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=1000)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "get_test_train_errors(adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m8ZPeeGHh46"
   },
   "source": [
    "Leave trees depth as 5, decrease n_estimators to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt4XLtgeHh46"
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=200)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "get_test_train_errors(adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HZehFD4Hh49"
   },
   "source": [
    "## Hello, overfitting!\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5jwLJEpHh4-"
   },
   "source": [
    "# Early stopping\n",
    "\n",
    "Ok, we understood, that we don't need to much trees: we overfit. But if we take not enough trees, we will overfit. Here comes the regularization.\n",
    "\n",
    "And one of the best regularization methods (best in practice), that is applied to ensembles (and to NN) is early stopping.\n",
    "\n",
    "## The idea behind it is simple: if our validation loss didn't increase last 'n_tolerance' steps, then we stop adding new trees.\n",
    "\n",
    "![](https://deeplearning4j.org/images/guide/earlystopping.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qdApGmTHh4_"
   },
   "source": [
    "Let's implement Early Stopping for adaboost algorithm. We will extend the class AdaBoostClassifier. In order to implement the Early Stopping, you should understand, that it is easier to cut the final algorithm (select only a number of first trees) instead of training it each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5VVNZ14Hh4_"
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingAdaBoost(AdaBoostClassifier):\n",
    "    def __init__(self, n_tolerance, score_func, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_tolerance = n_tolerance\n",
    "        self.score_func = score_func\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        # train big AdaBoost\n",
    "        super().fit(X_train, y_train)\n",
    "\n",
    "        val_scores = [] # validation scores of subsets of trees\n",
    "        wait = 1 # how much iterarations we have already waited for the increase of the score\n",
    "        n_trees = 0 # number of trees to take after early stopping\n",
    "        \n",
    "        \n",
    "        for y_pred in self.staged_predict(X_val): \n",
    "            # stage predict allows to predict using only subset of trees \n",
    "            # i.e. on the first iteration with 1 tree, on the second with 2 trees\n",
    "\n",
    "            # Now you have to write an early stopping algorithm\n",
    "            ######################\n",
    "            # YOUR CODE HERE!\n",
    "            \n",
    "\n",
    "            # YOUR CODE HERE!\n",
    "            ######################\n",
    "                    \n",
    "        self.estimators_ = self.estimators_[:n_trees]\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_nsmLkiHh5C"
   },
   "outputs": [],
   "source": [
    "adaboost = EarlyStoppingAdaBoost(base_estimator=DecisionTreeClassifier(max_depth=5),\n",
    "                                 n_estimators=200,\n",
    "                                 n_tolerance=10, score_func=f1_score)\n",
    "adaboost.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "get_test_train_errors(adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6_qaMzIHh5F"
   },
   "outputs": [],
   "source": [
    "adaboost = EarlyStoppingAdaBoost(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                                 n_estimators=1000,\n",
    "                                 n_tolerance=10, score_func=f1_score)\n",
    "adaboost.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "get_test_train_errors(adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMpuoifLHh5H"
   },
   "source": [
    "# Feature Importance: how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6Kq5lvyHh5J"
   },
   "outputs": [],
   "source": [
    "importances = adaboost.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in adaboost.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, data.columns[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1-NwEv5Hh5L"
   },
   "source": [
    "# Stacking: Voting\n",
    "\n",
    "Imagine you have two powerful algorithms, such as SVM and AdaBoost. You found the best hyperparameters for each of the algorithms. Usually in practice you can increase the prediction quality by combining two algorithms using stacking.\n",
    "\n",
    "One of the way to perform stacking is voting. If you have two algorithms $clf_1(x)$ and $clf_2(x)$, then the prediction will be \n",
    "\n",
    "$$result(x) = clf_1(x) * \\alpha + clf_2(x) * (1 - \\alpha),$$\n",
    "\n",
    "where $\\alpha \\in [0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQUHr0giHh5M"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMufwDrGHh5O"
   },
   "outputs": [],
   "source": [
    "clf1 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),\n",
    "                                 n_estimators=20)\n",
    "clf2 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                                 n_estimators=50)\n",
    "\n",
    "\n",
    "mix_clf_scores = []\n",
    "for alpha in np.linspace(0., 1., 10):\n",
    "    clf = VotingClassifier([('clf1', clf1), ('clf2', clf2)], voting='soft', weights=[alpha, 1-alpha])\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    f1 = f1_score(y_val, clf.predict(X_val))\n",
    "    \n",
    "    mix_clf_scores.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxGe1_9AHh5R"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0., 1., 10), mix_clf_scores)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('f1_score')\n",
    "plt.title('VotingClassifier scores depending from weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqq2YlQsHh5V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNUqLGCRHh5X"
   },
   "source": [
    "# Stacking: Meta-learner\n",
    "\n",
    "![](https://blogs.sas.com/content/subconsciousmusings/files/2017/05/modelstacking.png)\n",
    "\n",
    "Another possible solution for combination of several algorithms: learning some meta algorithm on the top of some base algorithms. For example, you have trained such algorithms, as SVM, Random Forest and Adaboost on your dataset. Each of these algorithms is able to return some scores, that you can use as features of the input objects. After it you can train some easy linear model (linear regression, logistic regression) on this features (which are the output of base algorithms).\n",
    "\n",
    "## What we can use as the information from the base algorithms?\n",
    "\n",
    "![](https://i.imgur.com/OrkSTqG.jpg)\n",
    "\n",
    "If you would like to construct such kind of stacking, you should be aware of overfitting. If you use the same train set for both base learners and meta learners, you will overfit.\n",
    "\n",
    "\n",
    "\n",
    "So, your task for now is to implement stacking. You can choose what to do yourself:\n",
    "\n",
    "* Use K-fold stacking\n",
    "* Split your data into train and calibration sets\n",
    "* Use train set for both training and calibration, and then obtain overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1Uh_5JmHh5Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.base import clone\n",
    "\n",
    "class MetaLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, base_learners, penalty='l2', dual=False, tol=0.0001,\n",
    "                 C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                 class_weight=None, random_state=None, solver='liblinear',\n",
    "                 max_iter=100, multi_class='ovr', verbose=0,\n",
    "                 warm_start=False, n_jobs=None):\n",
    "        super().__init__(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept,\n",
    "                         intercept_scaling=intercept_scaling, class_weight=class_weight,\n",
    "                         random_state=random_state, solver=solver, max_iter=max_iter,\n",
    "                         multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        self.base_learners = base_learners\n",
    "        \n",
    "        \n",
    "    def fit_predict_base_learners(self, _X, _y):\n",
    "        # write a function, that will feat each base classiffier and\n",
    "        # return predictions of base learners for the train set (M1, M2 in algorithm)\n",
    "        # HINT: from sklearn.model_selection import cross_val_predict\n",
    "        ######################\n",
    "        # YOUR CODE HERE!      \n",
    "\n",
    "\n",
    "        # YOUR CODE HERE!\n",
    "        ######################\n",
    "        \n",
    "    def fit_base_learners_full(self, _X, _y):\n",
    "        # write a function to fit a base learners on the whole(!) train set\n",
    "        ######################\n",
    "        # YOUR CODE HERE!\n",
    "\n",
    "\n",
    "        # YOUR CODE HERE!\n",
    "        ######################\n",
    "        \n",
    "    def get_base_predictions(self, _X):\n",
    "        # write a function, get the predictions of each base algorithm and return it\n",
    "        # in the form of matrix of shape [X.shape[0], n_base_algorithms]\n",
    "        ######################\n",
    "        # YOUR CODE HERE!\n",
    "\n",
    "\n",
    "        # YOUR CODE HERE!\n",
    "        ######################\n",
    "        \n",
    "    def fit(self, _X, _y):\n",
    "        # K-fold fitting of base learners\n",
    "        base_predictions = self.fit_predict_base_learners(_X, _y)\n",
    "        \n",
    "        # learn meta Logistic Regression\n",
    "        super().fit(base_predictions, _y)\n",
    "        \n",
    "        # fitting for test predictions\n",
    "        self.fit_base_learners_full(_X, _y)        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, _X):\n",
    "        base_predictions = self.get_base_predictions(_X)\n",
    "        return super().predict(base_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJOV_FjaHh5a"
   },
   "outputs": [],
   "source": [
    "clf1 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),\n",
    "                                 n_estimators=20)\n",
    "clf2 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                                 n_estimators=50)\n",
    "\n",
    "meta = MetaLogisticRegression(base_learners=[clf1, clf2])\n",
    "meta.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbKkxSYGHh5i"
   },
   "outputs": [],
   "source": [
    "f1_score(y_val, meta.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W6tMfiAHh5k"
   },
   "source": [
    "# Challenge\n",
    "\n",
    "Now let's check your imagination in designing ML algorithms. You task is to construct an algorithm in order to achive the highest f1 score on the dataset we considered today.\n",
    "\n",
    "You have X_train, X_val, X_test. You can train/validate on both X_train and X_val on your choice. X_test — this is only for test prediction.\n",
    "\n",
    "Key ingredients to success in Kaggle:\n",
    "* Boosting\n",
    "* Stacking\n",
    "* Feature Engineering (removing non-important features, constructing new features)\n",
    "* Data preprocessing (outliers)\n",
    "\n",
    "You may start with stacking of several algorithms of different nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3qMFAViHh5m"
   },
   "outputs": [],
   "source": [
    "def get_test_score(y_pred):\n",
    "    return f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4wD-ubiHh5p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwT88vZSHh5r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4aGtFZwHh5v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCoZM5gTHh5x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0HZehFD4Hh49"
   ],
   "name": "Adaboost2022_Seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
